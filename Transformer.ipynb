{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf86be3b",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68299c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR100\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "plt.set_cmap('cividis')\n",
    "set_matplotlib_formats('svg', 'pdf')\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "DATASET_PATH = os.environ.get('PATH_DATASETS', 'data/')\n",
    "CHECKPOINT_PATH = os.environ.get('PATH_CHECKPOINT', 'saved_models/Transformers/')\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "print('Device: ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n",
    "\n",
    "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
    "\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if '/' in file_name:\n",
    "        os.makedirs(file_path.rsplit('/', 1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f'Downloading {file_url}')\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file manually,\"\n",
    "                \" or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aa9b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
    "    attn_logits = attn_logits / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "pl.seed_everything(42)\n",
    "q = torch.randn(seq_len, d_k)\n",
    "k = torch.randn(seq_len, d_k)\n",
    "v = torch.randn(seq_len, d_k)\n",
    "\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f7219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self._reset_parameters()\n",
    "    \n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # num_heads 앞으로 옮김\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        values = values.reshape(batch_size, seq_length, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "        \n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58898ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "        \n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out = self.self_attn(x, mask=mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        linear_out = self.linear_net(x)\n",
    "        x = x + self.dropout(linear_out)\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d7730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, **block_args):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderBlock(**block_args) for _ in range(num_layers)])\n",
    "\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask=mask)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def get_attention_maps(self, x, mask=None):\n",
    "        attention_maps = []\n",
    "        for layer in self.layers:\n",
    "            _, attn_map = layer.self_attn(x, mask=mask, return_attention=True)\n",
    "            attention_maps.append(attn_map)\n",
    "            x = layer(x)\n",
    "        return attention_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        self.register_buffer('pe', pe, persistent=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "encod_block = PositionalEncoding(d_model=48, max_len=96)\n",
    "pe = encod_block.pe.squeeze().T.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8, 3))\n",
    "pos = ax.imshow(pe, cmap='RdGy', extent=(1, pe.shape[1] + 1, pe.shape[0] + 1, 1))\n",
    "fig.colorbar(pos, ax=ax)\n",
    "ax.set_xlabel(\"Position in sequence\")\n",
    "ax.set_ylabel(\"Hidden dimension\")\n",
    "ax.set_title(\"Positional encoding over hidden dimensions\")\n",
    "ax.set_xticks([1] + [i * 10 for i in range(1, 1 + pe.shape[1] // 10)])\n",
    "ax.set_yticks([1] + [i * 10 for i in range(1, 1 + pe.shape[0] // 10)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83426a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 4))\n",
    "ax = [a for a_list in ax for a in a_list]\n",
    "for i in range(len(ax)):\n",
    "    ax[i].plot(np.arange(1, 17), pe[i, :16], color=f'C{i}', marker='o', markersize=6, markeredgecolor='black')\n",
    "    ax[i].set_title(\"Encoding in hidden dimension %i\" % (i + 1))\n",
    "    ax[i].set_xlabel(\"Position in sequence\", fontsize=10)\n",
    "    ax[i].set_ylabel(\"Positional encoding\", fontsize=10)\n",
    "    ax[i].set_xticks(np.arange(1, 17))\n",
    "    ax[i].tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "    ax[i].tick_params(axis=\"both\", which=\"minor\", labelsize=8)\n",
    "    ax[i].set_ylim(-1.2, 1.2)\n",
    "fig.subplots_adjust(hspace=0.8)\n",
    "sns.reset_orig()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f05004",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineWarmupScheduler(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, warmup, max_iters):\n",
    "        self.warmup = warmup\n",
    "        self.max_num_iters = max_iters\n",
    "        super().__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr_factor = self.get_lr_factor(epoch=self.last_epoch)\n",
    "        return [base_lr * lr_factor for base_lr in self.base_lrs]\n",
    "\n",
    "    def get_lr_factor(self, epoch):\n",
    "        lr_factor = 0.5 * (1 + np.cos(np.pi * epoch / self.max_num_iters))\n",
    "        if epoch <= self.warmup:\n",
    "            lr_factor *= epoch * 1.0 / self.warmup\n",
    "        return lr_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebefbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed for initializing the lr scheduler\n",
    "p = nn.Parameter(torch.empty(4, 4))\n",
    "optimizer = optim.Adam([p], lr=1e-3)\n",
    "lr_scheduler = CosineWarmupScheduler(optimizer=optimizer, warmup=100, max_iters=2000)\n",
    "\n",
    "# Plotting\n",
    "epochs = list(range(2000))\n",
    "sns.set()\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(epochs, [lr_scheduler.get_lr_factor(e) for e in epochs])\n",
    "plt.ylabel(\"Learning rate factor\")\n",
    "plt.xlabel(\"Iterations (in batches)\")\n",
    "plt.title(\"Cosine Warm-up Learning Rate Scheduler\")\n",
    "plt.show()\n",
    "sns.reset_orig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e80b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerPredictor(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 model_dim,\n",
    "                 num_classes,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 lr,\n",
    "                 warmup,\n",
    "                 max_iters,\n",
    "                 dropout=0.0,\n",
    "                 input_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self._create_model()\n",
    "        \n",
    "    \n",
    "    def _create_model(self):\n",
    "        self.input_net = nn.Sequential(\n",
    "            nn.Dropout(self.hparams.input_dropout),\n",
    "            nn.Linear(self.hparams.input_dim, self.hparams.model_dim)\n",
    "        )\n",
    "        \n",
    "        self.positional_encoding = PositionalEncoding(d_model=self.hparams.model_dim)\n",
    "        \n",
    "        self.transformer = TransformerEncoder(\n",
    "            num_layers=self.hparams.num_layers,\n",
    "            input_dim=self.hparams.model_dim,\n",
    "            dim_feedforward=2 * self.hparams.model_dim,\n",
    "            num_heads=self.hparams.num_heads,\n",
    "            dropout=self.hparams.dropout,\n",
    "        )\n",
    "        \n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.model_dim),\n",
    "            nn.LayerNorm(self.hparams.model_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(self.hparams.dropout),\n",
    "            nn.Linear(self.hparams.model_dim, self.hparams.num_classes)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x, mask=None, add_positional_encoding=True):\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        x = self.transformer(x, mask=mask)\n",
    "        x = self.output_net(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def get_attention_maps(self, x, mask=None, add_positional_encoding=True):\n",
    "        x = self.input_net(x)\n",
    "        if add_positional_encoding:\n",
    "            x = self.positional_encoding(x)\n",
    "        attention_maps = self.transformer.get_attention_maps(x, mask=mask)\n",
    "        return attention_maps\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "        \n",
    "        self.lr_scheduler = CosineWarmupScheduler(\n",
    "            optimizer, warmup=self.hparams.warmup, max_iter=self.hparams.max_iters\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "    \n",
    "    def optimizer_step(self, *args, **kwargs):\n",
    "        super().optimizer_step(*agrs, **kwargs)\n",
    "        self.lr_scheduler.step()\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDataset(data.Dataset):\n",
    "    def __init__(self, num_categories, seq_len, size):\n",
    "        super().__init__()\n",
    "        self.num_categories = num_categories\n",
    "        self.seq_len = seq_len\n",
    "        self.size = size\n",
    "        \n",
    "        self.data = torch.randint(self.num_categories, size=(self.size, self.seq_len))\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp_data = self.data[idx]\n",
    "        labels = torch.flip(inp_data, dims=(0,))\n",
    "        return inp_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648c525",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = partial(ReverseDataset, 10, 16)\n",
    "train_loader = data.DataLoader(dataset(50000), batch_size=128, shuffle=True, drop_last=True, pin_memory=True)\n",
    "val_loader = data.DataLoader(dataset(1000), batch_size=128)\n",
    "test_loader = data.DataLoader(dataset(10000), batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a175bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, labels = train_loader.dataset[0]\n",
    "print(\"Input data:\", inp_data)\n",
    "print(\"Labels:    \", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6744e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReversePredictor(TransformerPredictor):\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        inp_data, labels = batch\n",
    "        inp_data = F.one_hot(inp_data, num_classes=self.hparams.num_classes).float()\n",
    "        \n",
    "        preds = self.forward(inp_data, add_positional_encoding=True)\n",
    "        loss = F.cross_entropy(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log(f'{mode}_loss', loss)\n",
    "        self.log(f'{mode}_acc', acc)\n",
    "        return loss, acc\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode='train')\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "    \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9a23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reverse(**kwargs):\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, 'ReverseTask')\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc')],\n",
    "        gpus=1,\n",
    "        max_epochs=10,\n",
    "        gradient_clip_val=5,\n",
    "        progress_bar_refresh_rate=1,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None\n",
    "    \n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'ReverseTask.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = ReversePredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = ReversePredictor(max_iters=trainer.max_epochs * len(train_loader), **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    \n",
    "    result = {\"test_acc\": test_result[0][\"test_acc\"], \"val_acc\": val_result[0][\"test_acc\"]}\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model, result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c063cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_model, reverse_result = train_reverse(\n",
    "    input_dim=train_loader.dataset.num_categories,\n",
    "    model_dim=32,\n",
    "    num_heads=1,\n",
    "    num_classes=train_loader.dataset.num_categories,\n",
    "    num_layers=1,\n",
    "    dropout=0.0,\n",
    "    lr=5e-4,\n",
    "    warmup=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val accuracy:  %4.2f%%\" % (100.0 * reverse_result[\"val_acc\"]))\n",
    "print(\"Test accuracy: %4.2f%%\" % (100.0 * reverse_result[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input, labels = next(iter(val_loader))\n",
    "inp_data = F.one_hot(data_input, num_classes=reverse_model.hparams.num_classes).float()\n",
    "inp_data = inp_data.to(device)\n",
    "attention_maps = reverse_model.get_attention_maps(inp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d7e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429cd12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_maps(input_data, attn_maps, idx=0):\n",
    "    if input_data is not None:\n",
    "        input_data = input_data[idx].detach().cpu().numpy()\n",
    "    else:\n",
    "        input_data = np.arange(attn_maps[0][idx].shape[-1])\n",
    "    attn_maps = [m[idx].detach().cpu().numpy() for m in attn_maps]\n",
    "\n",
    "    num_heads = attn_maps[0].shape[0]\n",
    "    num_layers = len(attn_maps)\n",
    "    seq_len = input_data.shape[0]\n",
    "    fig_size = 4 if num_heads == 1 else 3\n",
    "    fig, ax = plt.subplots(num_layers, num_heads, figsize=(num_heads * fig_size, num_layers * fig_size))\n",
    "    if num_layers == 1:\n",
    "        ax = [ax]\n",
    "    if num_heads == 1:\n",
    "        ax = [[a] for a in ax]\n",
    "    for row in range(num_layers):\n",
    "        for column in range(num_heads):\n",
    "            ax[row][column].imshow(attn_maps[row][column], origin=\"lower\", vmin=0)\n",
    "            ax[row][column].set_xticks(list(range(seq_len)))\n",
    "            ax[row][column].set_xticklabels(input_data.tolist())\n",
    "            ax[row][column].set_yticks(list(range(seq_len)))\n",
    "            ax[row][column].set_yticklabels(input_data.tolist())\n",
    "            ax[row][column].set_title(\"Layer %i, Head %i\" % (row + 1, column + 1))\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b11953",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention_maps(data_input, attention_maps, idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df93c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_MEANS = np.array([0.485, 0.456, 0.406])\n",
    "DATA_STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "TORCH_DATA_MEANS = torch.from_numpy(DATA_MEANS).view(1, 3, 1, 1)\n",
    "TORCH_DATA_STD = torch.from_numpy(DATA_STD).view(1, 3, 1, 1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "])\n",
    "\n",
    "train_set = CIFAR100(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "test_set = CIFAR100(root=DATASET_PATH, train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee26ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TORCH_HOME'] = CHECKPOINT_PATH\n",
    "pretrained_model = torchvision.models.resnet34(pretrained=True)\n",
    "\n",
    "pretrained_model.fc = nn.Sequential()\n",
    "pretrained_model.classifier = nn.Sequential()\n",
    "\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "pretrained_model.eval()\n",
    "for p in pretrained_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2d463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_features(dataset, save_file):\n",
    "    if not os.path.isfile(save_file):\n",
    "        data_loader = data.DataLoader(dataset, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "        extracted_features = []\n",
    "        for imgs, _ in tqdm(data_loader):\n",
    "            imgs = imgs.to(device)\n",
    "            feats = pretrained_model(imgs)\n",
    "            extracted_features.append(feats)\n",
    "        extracted_features = torch.cat(extracted_features, dim=0)\n",
    "        extracted_features = extracted_features.detach().cpu()\n",
    "        torch.save(extracted_features, save_file)\n",
    "    else:\n",
    "        extracted_features = torch.load(save_file)\n",
    "    return extracted_features\n",
    "\n",
    "\n",
    "train_feat_file = os.path.join(CHECKPOINT_PATH, \"train_set_features.tar\")\n",
    "train_set_feats = extract_features(train_set, train_feat_file)\n",
    "\n",
    "test_feat_file = os.path.join(CHECKPOINT_PATH, \"test_set_features.tar\")\n",
    "test_feats = extract_features(test_set, test_feat_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48431e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train:\", train_set_feats.shape)\n",
    "print(\"Test: \", test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdff8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train_set.targets\n",
    "\n",
    "labels = torch.LongTensor(labels)\n",
    "num_labels = labels.max() + 1\n",
    "sorted_indices = torch.argsort(labels).reshape(num_labels, -1)\n",
    "\n",
    "num_val_exmps = sorted_indices.shape[1] // 10\n",
    "\n",
    "# Get image indices for validation and training\n",
    "val_indices = sorted_indices[:, :num_val_exmps].reshape(-1)\n",
    "train_indices = sorted_indices[:, num_val_exmps:].reshape(-1)\n",
    "\n",
    "# Group corresponding image features and labels\n",
    "train_feats, train_labels = train_set_feats[train_indices], labels[train_indices]\n",
    "val_feats, val_labels = train_set_feats[val_indices], labels[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec68b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetAnomalyDataset(data.Dataset):\n",
    "    def __init__(self, img_feats, labels, set_size=10, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_feats: Tensor of shape [num_imgs, img_dim]. Represents the high-level features.\n",
    "            labels: Tensor of shape [num_imgs], containing the class labels for the images\n",
    "            set_size: Number of elements in a set. N-1 are sampled from one class, and one from another one.\n",
    "            train: If True, a new set will be sampled every time __getitem__ is called.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.img_feats = img_feats\n",
    "        self.labels = labels\n",
    "        self.set_size = set_size - 1  # The set size is here the size of correct images\n",
    "        self.train = train\n",
    "\n",
    "        # Tensors with indices of the images per class\n",
    "        self.num_labels = labels.max() + 1\n",
    "        self.img_idx_by_label = torch.argsort(self.labels).reshape(self.num_labels, -1)\n",
    "\n",
    "        if not train:\n",
    "            self.test_sets = self._create_test_sets()\n",
    "\n",
    "    def _create_test_sets(self):\n",
    "        # Pre-generates the sets for each image for the test set\n",
    "        test_sets = []\n",
    "        num_imgs = self.img_feats.shape[0]\n",
    "        np.random.seed(42)\n",
    "        test_sets = [self.sample_img_set(self.labels[idx]) for idx in range(num_imgs)]\n",
    "        test_sets = torch.stack(test_sets, dim=0)\n",
    "        return test_sets\n",
    "\n",
    "    def sample_img_set(self, anomaly_label):\n",
    "        \"\"\"Samples a new set of images, given the label of the anomaly.\n",
    "\n",
    "        The sampled images come from a different class than anomaly_label\n",
    "        \"\"\"\n",
    "        # Sample class from 0,...,num_classes-1 while skipping anomaly_label as class\n",
    "        set_label = np.random.randint(self.num_labels - 1)\n",
    "        if set_label >= anomaly_label:\n",
    "            set_label += 1\n",
    "\n",
    "        # Sample images from the class determined above\n",
    "        img_indices = np.random.choice(self.img_idx_by_label.shape[1], size=self.set_size, replace=False)\n",
    "        img_indices = self.img_idx_by_label[set_label, img_indices]\n",
    "        return img_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.img_feats.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anomaly = self.img_feats[idx]\n",
    "        if self.train:  # If train => sample\n",
    "            img_indices = self.sample_img_set(self.labels[idx])\n",
    "        else:  # If test => use pre-generated ones\n",
    "            img_indices = self.test_sets[idx]\n",
    "\n",
    "        # Concatenate images. The anomaly is always the last image for simplicity\n",
    "        img_set = torch.cat([self.img_feats[img_indices], anomaly[None]], dim=0)\n",
    "        indices = torch.cat([img_indices, torch.LongTensor([idx])], dim=0)\n",
    "        label = img_set.shape[0] - 1\n",
    "\n",
    "        # We return the indices of the images for visualization purpose. \"Label\" is the index of the anomaly\n",
    "        return img_set, indices, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70364025",
   "metadata": {},
   "outputs": [],
   "source": [
    "SET_SIZE = 10\n",
    "test_labels = torch.LongTensor(test_set.targets)\n",
    "\n",
    "train_anom_dataset = SetAnomalyDataset(train_feats, train_labels, set_size=SET_SIZE, train=True)\n",
    "val_anom_dataset = SetAnomalyDataset(val_feats, val_labels, set_size=SET_SIZE, train=False)\n",
    "test_anom_dataset = SetAnomalyDataset(test_feats, test_labels, set_size=SET_SIZE, train=False)\n",
    "\n",
    "train_anom_loader = data.DataLoader(\n",
    "    train_anom_dataset, batch_size=64, shuffle=True, drop_last=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "val_anom_loader = data.DataLoader(val_anom_dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_anom_loader = data.DataLoader(test_anom_dataset, batch_size=64, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_exmp(indices, orig_dataset):\n",
    "    images = [orig_dataset[idx][0] for idx in indices.reshape(-1)]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    images = images * TORCH_DATA_STD + TORCH_DATA_MEANS\n",
    "\n",
    "    img_grid = torchvision.utils.make_grid(images, nrow=SET_SIZE, normalize=True, pad_value=0.5, padding=16)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(\"Anomaly examples on CIFAR100\")\n",
    "    plt.imshow(img_grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "_, indices, _ = next(iter(test_anom_loader))\n",
    "visualize_exmp(indices[:4], test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c35219",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyPredictor(TransformerPredictor):\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        img_sets, _, labels = batch\n",
    "        # No positional encodings as it is a set, not a sequence!\n",
    "        preds = self.forward(img_sets, add_positional_encoding=False)\n",
    "        preds = preds.squeeze(dim=-1)  # Shape: [Batch_size, set_size]\n",
    "        loss = F.cross_entropy(preds, labels)  # Softmax/CE over set dimension\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        self.log(\"%s_loss\" % mode, loss)\n",
    "        self.log(\"%s_acc\" % mode, acc, on_step=False, on_epoch=True)\n",
    "        return loss, acc\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, _ = self._calculate_loss(batch, mode=\"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _ = self._calculate_loss(batch, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly(**kwargs):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask\")\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=root_dir,\n",
    "        callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "        gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "        max_epochs=100,\n",
    "        gradient_clip_val=2,\n",
    "        progress_bar_refresh_rate=1,\n",
    "    )\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"SetAnomalyTask.ckpt\")\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = AnomalyPredictor.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = AnomalyPredictor(max_iters=trainer.max_epochs * len(train_anom_loader), **kwargs)\n",
    "        trainer.fit(model, train_anom_loader, val_anom_loader)\n",
    "        model = AnomalyPredictor.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, test_dataloaders=train_anom_loader, verbose=False)\n",
    "    val_result = trainer.test(model, test_dataloaders=val_anom_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_anom_loader, verbose=False)\n",
    "    result = {\n",
    "        \"test_acc\": test_result[0][\"test_acc\"],\n",
    "        \"val_acc\": val_result[0][\"test_acc\"],\n",
    "        \"train_acc\": train_result[0][\"test_acc\"],\n",
    "    }\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1816a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_model, anomaly_result = train_anomaly(\n",
    "    input_dim=train_anom_dataset.img_feats.shape[-1],\n",
    "    model_dim=256,\n",
    "    num_heads=4,\n",
    "    num_classes=1,\n",
    "    num_layers=4,\n",
    "    dropout=0.1,\n",
    "    input_dropout=0.1,\n",
    "    lr=5e-4,\n",
    "    warmup=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy: %4.2f%%\" % (100.0 * anomaly_result[\"train_acc\"]))\n",
    "print(\"Val accuracy:   %4.2f%%\" % (100.0 * anomaly_result[\"val_acc\"]))\n",
    "print(\"Test accuracy:  %4.2f%%\" % (100.0 * anomaly_result[\"test_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3771cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_data, indices, labels = next(iter(test_anom_loader))\n",
    "inp_data = inp_data.to(device)\n",
    "\n",
    "anomaly_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = anomaly_model.forward(inp_data, add_positional_encoding=False)\n",
    "    preds = F.softmax(preds.squeeze(dim=-1), dim=-1)\n",
    "\n",
    "    # Permut input data\n",
    "    permut = np.random.permutation(inp_data.shape[1])\n",
    "    perm_inp_data = inp_data[:, permut]\n",
    "    perm_preds = anomaly_model.forward(perm_inp_data, add_positional_encoding=False)\n",
    "    perm_preds = F.softmax(perm_preds.squeeze(dim=-1), dim=-1)\n",
    "\n",
    "assert (preds[:, permut] - perm_preds).abs().max() < 1e-5, \"Predictions are not permutation equivariant\"\n",
    "\n",
    "print(\"Preds\\n\", preds[0, permut].cpu().numpy())\n",
    "print(\"Permuted preds\\n\", perm_preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958adb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps = anomaly_model.get_attention_maps(inp_data, add_positional_encoding=False)\n",
    "predictions = preds.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0ee7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prediction(idx):\n",
    "    visualize_exmp(indices[idx : idx + 1], test_set)\n",
    "    print(\"Prediction:\", predictions[idx].item())\n",
    "    plot_attention_maps(input_data=None, attn_maps=attention_maps, idx=idx)\n",
    "\n",
    "\n",
    "visualize_prediction(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c50223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakes = torch.where(predictions != 9)[0].cpu().numpy()\n",
    "print(\"Indices with mistake:\", mistakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76422ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_prediction(mistakes[-1])\n",
    "print(\"Probabilities:\")\n",
    "for i, p in enumerate(preds[mistakes[-1]].cpu().numpy()):\n",
    "    print(\"Image %i: %4.2f%%\" % (i, 100.0 * p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233a107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
