{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb0672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from IPython.display import set_matplotlib_formats\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "# %matplotlib inline\n",
    "set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data/\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/VisionTransformers/\")\n",
    "\n",
    "# Setting the seed\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b664d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/\"\n",
    "# Files to download\n",
    "pretrained_files = [\n",
    "    \"tutorial15/ViT.ckpt\",\n",
    "    \"tutorial15/tensorboards/ViT/events.out.tfevents.ViT\",\n",
    "    \"tutorial5/tensorboards/ResNet/events.out.tfevents.resnet\",\n",
    "]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name.split(\"/\", 1)[1])\n",
    "    if \"/\" in file_name.split(\"/\", 1)[1]:\n",
    "        os.makedirs(file_path.rsplit(\"/\", 1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(\"Downloading %s...\" % file_url)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e993fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784])\n",
    "])\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.49139968, 0.48215841, 0.44653091], [0.24703223, 0.24348513, 0.26158784]),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=train_transform, download=True)\n",
    "val_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=test_transform, download=True)\n",
    "pl.seed_everything(42)\n",
    "train_set, _ = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "pl.seed_everything(42)\n",
    "_, val_set = torch.utils.data.random_split(val_dataset, [45000, 5000])\n",
    "\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=test_transform, download=True)\n",
    "\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "NUM_IMAGES = 4\n",
    "CIFAR_images = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
    "img_grid = torchvision.utils.make_grid(CIFAR_images, nrow=4, normalize=True, pad_value=0.9)\n",
    "img_grid = img_grid.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.title(\"Image examples of the CIFAR10 dataset\")\n",
    "plt.imshow(img_grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a5a8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size, flatten_channels=True):\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5)\n",
    "    x = x.flatten(1, 2)\n",
    "    if flatten_channels:\n",
    "        x = x.flatten(2, 4)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f2c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_patches = img_to_patch(CIFAR_images, patch_size=4, flatten_channels=False)\n",
    "\n",
    "fig, ax = plt.subplots(CIFAR_images.shape[0], 1, figsize=(14, 3))\n",
    "fig.suptitle(\"Images as input sequences of patches\")\n",
    "for i in range(CIFAR_images.shape[0]):\n",
    "    img_grid = torchvision.utils.make_grid(img_patches[i], nrow=64, normalize=True, pad_value=0.9)\n",
    "    img_grid = img_grid.permute(1, 2, 0)\n",
    "    ax[i].imshow(img_grid)\n",
    "    ax[i].axis(\"off\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c805a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        attn_x, attn_score = self.attn(inp_x, inp_x, inp_x)\n",
    "        x = x + attn_x\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72644f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim,\n",
    "                 hidden_dim,\n",
    "                 num_channels,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 num_classes,\n",
    "                 patch_size,\n",
    "                 num_patches,\n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        self.input_layer = nn.Linear(num_channels * (patch_size ** 2), embed_dim)\n",
    "        self.transformer = nn.Sequential(\n",
    "            *(AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers))\n",
    "        )\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "        \n",
    "        cls_token = self.cls_token.repeat(B, 1, 1)\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        x = x + self.pos_embedding[:, : T + 1]\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7866ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(pl.LightningModule):\n",
    "    def __init__(self, model_kwargs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VisionTransformer(**model_kwargs)\n",
    "        self.example_input_array = next(iter(train_loader))[0]\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr)\n",
    "        lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "    \n",
    "\n",
    "    def _calculate_loss(self, batch, mode='train'):\n",
    "        imgs, labels = batch\n",
    "        preds = self.model(imgs)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        acc = (preds.argmax(dim=-1) == labels).float().mean()\n",
    "        \n",
    "        self.log(f'{mode}_loss', loss)\n",
    "        self.log(f'{mode}_acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._calculate_loss(batch, mode='train')\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='val')\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._calculate_loss(batch, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c377cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(**kwargs):\n",
    "    trainer = pl.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, 'ViT'),\n",
    "        gpus=1,\n",
    "        max_epochs=100,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc'),\n",
    "            LearningRateMonitor('epoch')\n",
    "        ],\n",
    "        progress_bar_refresh_rate=1\n",
    "    )\n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric = None\n",
    "    \n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'ViT.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model at %s, loading...\" % pretrained_filename)\n",
    "        # Automatically loads the model with the saved hyperparameters\n",
    "        model = ViT.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = ViT(**kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = ViT.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "        \n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0][\"test_acc\"], \"val\": val_result[0][\"test_acc\"]}\n",
    "    \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, results = train_model(\n",
    "    model_kwargs={\n",
    "        \"embed_dim\": 256,\n",
    "        \"hidden_dim\": 512,\n",
    "        \"num_heads\": 8,\n",
    "        \"num_layers\": 6,\n",
    "        \"patch_size\": 4,\n",
    "        \"num_channels\": 3,\n",
    "        \"num_patches\": 64,\n",
    "        \"num_classes\": 10,\n",
    "        \"dropout\": 0.2,\n",
    "    },\n",
    "    lr=3e-4,\n",
    ")\n",
    "print(\"ViT results\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06c16e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
